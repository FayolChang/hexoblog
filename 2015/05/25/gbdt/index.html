<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>gbdt | Machine Learning, Algorithms and others</title>
  <meta name="author" content="Fayou Zhang">

  
  <meta name="description" content="GBDT关于机器学习，我目前看到的最好的中文书籍是《统计学习方法》，讲解的清晰易懂，而且有算法。看了之后都没有写东西的冲动了。
关于 GBDT，在该书中是放在第八章提升方法提升树这一节。该章首先介绍了 adaboost，关键是每次调整样本的权重，预测对了权重就调低，错了就调高。如何实现这一点呢？用得">
  
  

  <link rel="alternate" href="/hexoblog/atom.xml" title="Machine Learning, Algorithms and others" type="application/atom+xml">
  <link rel="stylesheet" href="/hexoblog/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script>
  
</head>

<body>
  <header id="header" class="inner"><nav>
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
</nav></header>
  <div id="content" class="inner"><article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <header>
    
  
    <h1 class="title">gbdt</h1>
  

    <time datetime="2015-05-25T13:41:11.000Z">
  <span class="day">25</span><span class="month">May</span>
</time>
  </header>
  <div class="entry-content">
    
      <h1 id="GBDT">GBDT</h1><p>关于机器学习，我目前看到的最好的中文书籍是《统计学习方法》，讲解的清晰易懂，而且有算法。看了之后都没有写东西的冲动了。</p>
<pre><code>关于 GBDT，在该书中是放在第八章提升方法提升树这一节。该章首先介绍了 adaboost，关键是每次调整样本的权重，预测对了权重就调低，错了就调高。如何实现这一点呢？用得是
</code></pre><p>$$<br>\alpha_m = \frac{1}{2}log \frac{1-e_m}{e_m}<br>$$<br>其中$e_m$是预测错误率。</p>
<p>然后介绍的是前向分步算法，是一个解决加法模型的算法。紧接着就证明了，如果损失函数取指数损失函数的话，那么能得出 adaboost，也就是说，adaboost 是前向分步算法的一个特列。</p>
<p>紧接着，在提升树算法中，如果采用平方误差损失的话，损失变成了</p>
<p>$$<br>L(y,f<em>{m-1}(x)+T(x;\Theta_m))=[y-f</em>{m-1}(x)-T(x;\Theta_m)]^2=[r-T(x;\Theta_m)]^2<br>$$</p>
<p>其中，r 是当前模型拟合的残差</p>
<p>所以，对于回归问题的提升树来说，只需拟合当前模型的残差即可。</p>
<p>我觉得这很有意思，以前是要修改样本的权重，到现在是要拟合残差，和样本权重好像没关系了</p>
<p>最后介绍的是梯度提升，说如果的是利用损失函数的负梯度在当前模型的值<br>$$<br>-\bigg[\frac{\partial L(y,\;f(x<em>{i}))}{\partial f(x</em>{i})}\bigg]<em>{f(x)=f</em>{m-1}(x)}<br>$$<br>来作为回归问题提升树算法中得残差近似值。</p>
<p>这里给出了 R 的<a href="http://xccds1977.blogspot.com/2015/05/boosting.html" target="_blank" rel="external">博客</a></p>
<p><a href="https://gist.githubusercontent.com/xccds/432b8752a2f9c14e3148/raw/gbm.R" target="_blank" rel="external">代码</a></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Learn Gradient Boosting Model by coding</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># good slide</span></span><br><span class="line"><span class="comment"># http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf</span></span><br><span class="line"><span class="comment"># http://cran.r-project.org/web/packages/gbm/gbm.pdf</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1 Gradient Boosting for Regression</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># generate data</span></span><br><span class="line">generate_func = <span class="keyword">function</span>(n=<span class="number">1000</span>,size=<span class="number">0.5</span>)&#123;</span><br><span class="line">  <span class="comment"># n: how many data points</span></span><br><span class="line">  <span class="comment"># size: hold 50% of all data as train </span></span><br><span class="line">  set.seed(<span class="number">1</span>)</span><br><span class="line">  x = seq(<span class="number">0</span>,<span class="number">2</span>*pi,length=n) <span class="comment"># generate x</span></span><br><span class="line">  noise = rnorm(n,<span class="number">0</span>,<span class="number">0.3</span>)</span><br><span class="line">  y = sin(x) + noise <span class="comment"># generate y</span></span><br><span class="line">  select_index = sample(<span class="number">1</span>:n,size=size*n,replace = <span class="literal">F</span>)</span><br><span class="line">  <span class="comment"># split data into train and test</span></span><br><span class="line">  train_x = x[select_index]</span><br><span class="line">  train_y = y[select_index]</span><br><span class="line">  test_x = x[-select_index]</span><br><span class="line">  test_y = y[-select_index]</span><br><span class="line">  data = list(<span class="string">'train_x'</span>=train_x,</span><br><span class="line">             <span class="string">'train_y'</span>=train_y,</span><br><span class="line">             <span class="string">'test_x'</span>=test_x,</span><br><span class="line">             <span class="string">'test_y'</span>=test_y)</span><br><span class="line">  <span class="keyword">return</span>(data)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = generate_func()</span><br><span class="line">objects(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train boosting regression tree</span></span><br><span class="line">GBR = <span class="keyword">function</span>(x,y,rate=<span class="number">1</span>,iter=<span class="number">100</span>)&#123;</span><br><span class="line">  <span class="comment"># iter is iterate number, higher is better, but be carefule overfit.</span></span><br><span class="line">  <span class="comment"># rate is learning speed, lower is better, but too slow will cause longer time. </span></span><br><span class="line">  <span class="keyword">library</span>(rpart)</span><br><span class="line">  max_depth=<span class="number">1</span> <span class="comment"># tree stump</span></span><br><span class="line">  mu = mean(y) <span class="comment"># start with an initial model</span></span><br><span class="line">  dy = y - mu  <span class="comment"># residuals or error,  These are the parts that existing model cannot do well.</span></span><br><span class="line">  learner = list() <span class="comment"># define a learners holder </span></span><br><span class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:iter) &#123;</span><br><span class="line">    <span class="comment"># use a weak model to improve</span></span><br><span class="line">    learner[[i]] = rpart(dy~x, <span class="comment"># error is target variable</span></span><br><span class="line">                         control=rpart.control(maxdepth=max_depth,cp=<span class="number">0</span>)) <span class="comment"># cp=0 means to growth a tree with any cost</span></span><br><span class="line">    <span class="comment"># modify residuals for next iter</span></span><br><span class="line">    dy = dy - rate*predict(learner[[i]])</span><br><span class="line">  &#125;</span><br><span class="line">  result = list(<span class="string">'learner'</span>=learner,</span><br><span class="line">                <span class="string">'rate'</span>=rate,</span><br><span class="line">                <span class="string">'iter'</span>=iter)</span><br><span class="line">  <span class="keyword">return</span>(result)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = GBR(data$train_x,data$train_y,iter=<span class="number">1000</span>)</span><br><span class="line">objects(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict function</span></span><br><span class="line">predict_GBR = <span class="keyword">function</span>(x,y,model)&#123;</span><br><span class="line">  predict_y = list() <span class="comment"># hold predict result</span></span><br><span class="line">  mu = mean(y) <span class="comment"># initial model</span></span><br><span class="line">  n = length(y)</span><br><span class="line">  iter = model$iter</span><br><span class="line">  rate = model$rate</span><br><span class="line">  predict_y[[<span class="number">1</span>]] = rep(mu,n)</span><br><span class="line">  learner = model$learner</span><br><span class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">2</span>:iter) &#123;</span><br><span class="line">    <span class="comment"># add pre-model prediction to predict y</span></span><br><span class="line">    predict_y[[i]] = predict_y[[i-<span class="number">1</span>]] + rate * predict(learner[[i-<span class="number">1</span>]],newdata=data.frame(x))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment"># mean sqare error</span></span><br><span class="line">  mse = sapply(predict_y,<span class="keyword">function</span>(pre_y) round(mean((y-pre_y)^<span class="number">2</span>),<span class="number">3</span>))</span><br><span class="line">  result = list(<span class="string">'predict_y'</span>=predict_y,</span><br><span class="line">                <span class="string">'mse'</span>= mse)</span><br><span class="line">  <span class="keyword">return</span>(result)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict train data</span></span><br><span class="line">predict_train = predict_GBR(data$train_x,data$train_y,model=model)</span><br><span class="line">objects(predict_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># more trees get less error</span></span><br><span class="line">plot(predict_train$mse,type=<span class="string">'l'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the effect of boosing tree</span></span><br><span class="line">plotfunc = <span class="keyword">function</span>(x,y,predict,num)&#123;</span><br><span class="line">  <span class="keyword">library</span>(ggplot2)</span><br><span class="line">  pre = predict$predict_y[[num]] </span><br><span class="line">  plotdf = data.frame(x=x,y=y,pre = pre)</span><br><span class="line">  mse = round(mean((y-pre)^<span class="number">2</span>),<span class="number">3</span>)</span><br><span class="line">  p = ggplot(plotdf,aes(x,y)) +</span><br><span class="line">    geom_point(alpha=<span class="number">0.5</span>)</span><br><span class="line">  p = p + geom_line(aes(x,pre)) + xlab(paste0(<span class="string">'mse='</span>,mse))</span><br><span class="line">  plot(p)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># use mean of y to predict data</span></span><br><span class="line">plotfunc(data$train_x,data$train_y,predict_train,num=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># add another tree model</span></span><br><span class="line">plotfunc(data$train_x,data$train_y,predict_train,num=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># add more tree getter more result</span></span><br><span class="line">plotfunc(data$train_x,data$train_y,predict_train,num=<span class="number">10</span>)</span><br><span class="line">plotfunc(data$train_x,data$train_y,predict_train,num=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict test data</span></span><br><span class="line">predict_test = predict_GBR(data$test_x,data$test_y,model=model)</span><br><span class="line">plot(predict_test$mse,type=<span class="string">'l'</span>)</span><br><span class="line"></span><br><span class="line">plotfunc(data$test_x,data$test_y,predict_test,<span class="number">10</span>)</span><br><span class="line">plotfunc(data$test_x,data$test_y,predict_test,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compare different parametter</span></span><br><span class="line"><span class="comment"># create 2 models with different rate</span></span><br><span class="line">model_1 = GBR(data$train_x,data$train_y,rate=<span class="number">1</span>,iter=<span class="number">500</span>)</span><br><span class="line">model_2 = GBR(data$train_x,data$train_y,rate=<span class="number">0.1</span>,iter=<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use train and test data, we have 4 results</span></span><br><span class="line">predict_train_1 = predict_GBR(data$train_x,data$train_y,model=model_1)</span><br><span class="line">predict_train_2 = predict_GBR(data$train_x,data$train_y,model=model_2)</span><br><span class="line">predict_test_1 = predict_GBR(data$test_x,data$test_y,model=model_1)</span><br><span class="line">predict_test_2 = predict_GBR(data$test_x,data$test_y,model=model_2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># take out mse of these 4 results</span></span><br><span class="line">train_error_1 = predict_train_1$mse</span><br><span class="line">train_error_2 = predict_train_2$mse</span><br><span class="line">test_error_1 = predict_test_1$mse</span><br><span class="line">test_error_2 = predict_test_2$mse</span><br><span class="line">iter = <span class="number">1</span>:model_1$iter</span><br><span class="line"></span><br><span class="line"><span class="comment"># compare these mse</span></span><br><span class="line">plotdf = data.frame(iter,train_error_1,test_error_1,train_error_2,test_error_2)</span><br><span class="line"></span><br><span class="line">p = ggplot(plotdf)+</span><br><span class="line">    geom_line(aes(x=iter,y=train_error_1),color=<span class="string">'blue'</span>)+</span><br><span class="line">    geom_line(aes(x=iter,y=test_error_1),color=<span class="string">'red'</span>) +</span><br><span class="line">  geom_line(aes(x=iter,y=train_error_2),linetype=<span class="number">2</span>,color=<span class="string">'blue'</span>)+</span><br><span class="line">  geom_line(aes(x=iter,y=test_error_2),linetype=<span class="number">2</span>,color=<span class="string">'red'</span>)</span><br><span class="line">print(p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># test error is better model performance.</span></span><br><span class="line"><span class="comment"># less rate is better but need more iter</span></span><br><span class="line">which.min(train_error_1)</span><br><span class="line">which.min(train_error_2)</span><br><span class="line">which.min(test_error_1)</span><br><span class="line">which.min(test_error_2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Gradient Boosting for Classification</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># read data</span></span><br><span class="line">data = subset(iris,Species!=<span class="string">'virginica'</span>,select = c(<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>))</span><br><span class="line">data$y = ifelse(data$Species == <span class="string">'setosa'</span>,<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">data$Species = <span class="literal">NULL</span></span><br><span class="line">names(data)[<span class="number">1</span>:<span class="number">2</span>] = c(<span class="string">'x1'</span>,<span class="string">'x2'</span>)</span><br><span class="line">head(data)</span><br><span class="line">p = ggplot(data,aes(x1,x2,color=factor(y)))+</span><br><span class="line">    geom_point()</span><br><span class="line">print(p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train boosting tree for classification</span></span><br><span class="line">GBC = <span class="keyword">function</span>(data,rate=<span class="number">0.1</span>,iter=<span class="number">100</span>)&#123;</span><br><span class="line">  <span class="keyword">library</span>(rpart)</span><br><span class="line">  max_depth=<span class="number">1</span></span><br><span class="line">  learner = list()</span><br><span class="line">  mu = mean(data$y==<span class="number">1</span>)</span><br><span class="line">  <span class="comment"># start with an initial model</span></span><br><span class="line">  <span class="comment"># mu=p(y=1) -&gt; f=w*x = log(mu/(1-mu)) </span></span><br><span class="line">  f = log(mu/(<span class="number">1</span>-mu)) </span><br><span class="line">  data$dy = data$y/(<span class="number">1</span>+exp(data$y*f)) <span class="comment"># dy is negtive gradient of log loss funtion</span></span><br><span class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:iter) &#123;</span><br><span class="line">    <span class="comment"># use a weak model to improve</span></span><br><span class="line">    learner[[i]] = rpart(dy~x1+x2,data=data,</span><br><span class="line">                         control=rpart.control(maxdepth=max_depth,cp=<span class="number">0</span>))</span><br><span class="line">    <span class="comment"># improve model </span></span><br><span class="line">    f = f + rate *predict(learner[[i]])</span><br><span class="line">    <span class="comment"># modify dy</span></span><br><span class="line">    data$dy = data$y/(<span class="number">1</span>+exp(data$y*f))</span><br><span class="line">  &#125;</span><br><span class="line">  result = list(<span class="string">'learner'</span>=learner,</span><br><span class="line">                <span class="string">'rate'</span>=rate,</span><br><span class="line">                <span class="string">'iter'</span>=iter)</span><br><span class="line">  <span class="keyword">return</span>(result)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model = GBC(data,rate=<span class="number">0.1</span>,iter=<span class="number">1000</span>)</span><br><span class="line">objects(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict function</span></span><br><span class="line">predict_GBC = <span class="keyword">function</span>(data,model)&#123;</span><br><span class="line">  predict_y = list()</span><br><span class="line">  mu = mean(data$y==<span class="number">1</span>)</span><br><span class="line">  f = log(mu/(<span class="number">1</span>-mu)) </span><br><span class="line">  n = nrow(data)</span><br><span class="line">  iter = model$iter</span><br><span class="line">  rate = model$rate</span><br><span class="line">  predict_y[[<span class="number">1</span>]] = rep(f,n)</span><br><span class="line">  learner = model$learner</span><br><span class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">2</span>:iter) &#123;</span><br><span class="line">    predict_y[[i]] = predict_y[[i-<span class="number">1</span>]] + rate *predict(learner[[i-<span class="number">1</span>]],newdata=data)</span><br><span class="line">  &#125;</span><br><span class="line">  mse = sapply(predict_y,<span class="keyword">function</span>(pre_y) sum(log(<span class="number">1</span>+exp(-data$y*pre_y)))) <span class="comment"># logistic loss function</span></span><br><span class="line">  result = list(<span class="string">'predict_y'</span>=predict_y,</span><br><span class="line">                <span class="string">'mse'</span>= mse)</span><br><span class="line">  <span class="keyword">return</span>(result)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict data</span></span><br><span class="line">predict_train = predict_GBC(data,model=model)</span><br><span class="line">objects(predict_train)</span><br><span class="line">plot(predict_train$mse,type=<span class="string">'l'</span>)</span><br><span class="line"></span><br><span class="line">final = predict_train$predict_y[[<span class="number">1000</span>]]</span><br><span class="line">y_p = <span class="number">1</span>/(<span class="number">1</span>+exp(-final))</span><br><span class="line">y_pred = ifelse(y_p&gt;<span class="number">0.5</span>,<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">table(data$y, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # plot</span></span><br><span class="line"></span><br><span class="line">plotfunc2 = <span class="keyword">function</span>(data,num,rate=<span class="number">0.1</span>)&#123;</span><br><span class="line">  <span class="keyword">library</span>(ggplot2)</span><br><span class="line">  model = GBC(data,rate=rate,iter=num)</span><br><span class="line">  predict_train = predict_GBC(data,model=model)</span><br><span class="line">  final = predict_train$predict_y[[num]]</span><br><span class="line">  y_p = <span class="number">1</span>/(<span class="number">1</span>+exp(-final))</span><br><span class="line">  data$y_pre = ifelse(y_p&gt;<span class="number">0.5</span>,<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">  tree_f = sapply(model$learner,<span class="keyword">function</span>(x)&#123;</span><br><span class="line">    temp = x$splits</span><br><span class="line">    <span class="keyword">return</span>(row.names(temp)[<span class="number">1</span>])</span><br><span class="line">  &#125;)</span><br><span class="line">  tree_v = sapply(model$learner,<span class="keyword">function</span>(x)&#123;</span><br><span class="line">    temp = x$splits</span><br><span class="line">    <span class="keyword">return</span>(temp[<span class="number">1</span>,<span class="string">'index'</span>])</span><br><span class="line">  &#125;)</span><br><span class="line">  x1value = tree_v[tree_f==<span class="string">'x1'</span>]</span><br><span class="line">  x2value = tree_v[tree_f==<span class="string">'x2'</span>]</span><br><span class="line">  p = ggplot(data,aes(x=x1,y=x2))</span><br><span class="line">  p = p + geom_point(aes(color=factor(y_pre)),size=<span class="number">5</span>) +</span><br><span class="line">    geom_point(aes(color=factor(y)),size=<span class="number">3</span>)+</span><br><span class="line">    geom_vline(xintercept = x1value,alpha=<span class="number">0.4</span>) +</span><br><span class="line">    geom_hline(yintercept = x2value,alpha=<span class="number">0.4</span>) +  </span><br><span class="line">    scale_colour_brewer(type=<span class="string">"seq"</span>, palette=<span class="string">'Set1'</span>) +</span><br><span class="line">    theme_bw()</span><br><span class="line">  print(p)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plotfunc2(data,num=<span class="number">10</span>)</span><br><span class="line">plotfunc2(data,num=<span class="number">20</span>)</span><br><span class="line">plotfunc2(data,num=<span class="number">60</span>)</span><br><span class="line">plotfunc2(data,num=<span class="number">100</span>)</span><br><span class="line">plotfunc2(data,num=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
    
    
    <footer class="meta">
      
      
  <div class="tags">
<a href="/hexoblog/tags/boosting/">boosting</a><a href="/hexoblog/tags/machine-learning/">machine learning</a></div>

      
    </footer>
    
  </div>
  
</article></div>
  <footer id="footer" class="inner"><div class="social alignright">
  
  
  
  
  <a class="rss" href="/hexoblog/atom.xml" title="RSS">RSS</a>
</div>
<p>
  
  &copy; 2015 Fayou Zhang
  
</p>
<div class="clearfix"></div></footer>
  <script src="/hexoblog/js/jquery.imagesloaded.min.js"></script>
<script src="/hexoblog/js/gallery.js"></script>




<link rel="stylesheet" href="/hexoblog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/hexoblog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="phasebeam">
  <canvas></canvas>
  <canvas></canvas>
  <canvas></canvas>
</div>
<script src="/hexoblog/js/phasebeam.js"></script>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>