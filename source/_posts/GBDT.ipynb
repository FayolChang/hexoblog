{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于机器学习，我目前看到的最好的中文书籍是《统计学习方法》，讲解的清晰易懂，而且有算法。看了之后都没有写东西的冲动了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    关于 GBDT，在该书中是放在第八章提升方法提升树这一节。该章首先介绍了 adaboost，关键是每次调整样本的权重，预测对了权重就调低，错了就调高。如何实现这一点呢？用得是\n",
    "$$\n",
    "\\alpha_m = \\frac{1}{2}log \\frac{1-e_m}{e_m}\n",
    "$$\n",
    "其中$e_m$是预测错误率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后介绍的是前向分步算法，是一个解决加法模型的算法。紧接着就证明了，如果损失函数取指数损失函数的话，那么能得出 adaboost，也就是说，adaboost 是前向分步算法的一个特列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "紧接着，在提升树算法中，如果采用平方误差损失的话，损失变成了\n",
    "$$\n",
    "L(y,f_{m-1}(x)+T(x;\\Theta_m))=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2=[r-T(x;\\Theta_m)]^2\n",
    "$$\n",
    "其中，r 是当前模型拟合的残差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，对于回归问题的提升树来说，只需拟合当前模型的残差即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我觉得这很有意思，以前是要修改样本的权重，到现在是要拟合残差，和样本权重好像没关系了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后介绍的是梯度提升，说如果的是利用损失函数的负梯度在当前模型的值\n",
    "$$\n",
    "-\\bigg[\\frac{\\partial L(y,\\;f(x_{i}))}{\\partial f(x_{i})}\\bigg]_{f(x)=f_{m-1}(x)}\n",
    "$$\n",
    "来作为回归问题提升树算法中得残差近似值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里给出了 R 的代码 http://xccds1977.blogspot.com/2015/05/boosting.html\n",
    "\n",
    "https://gist.githubusercontent.com/xccds/432b8752a2f9c14e3148/raw/gbm.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learn Gradient Boosting Model by coding\n",
    "\n",
    "# good slide\n",
    "# http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf\n",
    "# http://cran.r-project.org/web/packages/gbm/gbm.pdf\n",
    "\n",
    "#1 Gradient Boosting for Regression\n",
    "\n",
    "# generate data\n",
    "generate_func = function(n=1000,size=0.5){\n",
    "  # n: how many data points\n",
    "  # size: hold 50% of all data as train \n",
    "  set.seed(1)\n",
    "  x = seq(0,2*pi,length=n) # generate x\n",
    "  noise = rnorm(n,0,0.3)\n",
    "  y = sin(x) + noise # generate y\n",
    "  select_index = sample(1:n,size=size*n,replace = F)\n",
    "  # split data into train and test\n",
    "  train_x = x[select_index]\n",
    "  train_y = y[select_index]\n",
    "  test_x = x[-select_index]\n",
    "  test_y = y[-select_index]\n",
    "  data = list('train_x'=train_x,\n",
    "             'train_y'=train_y,\n",
    "             'test_x'=test_x,\n",
    "             'test_y'=test_y)\n",
    "  return(data)\n",
    "}\n",
    "\n",
    "data = generate_func()\n",
    "objects(data)\n",
    "\n",
    "# train boosting regression tree\n",
    "GBR = function(x,y,rate=1,iter=100){\n",
    "  # iter is iterate number, higher is better, but be carefule overfit.\n",
    "  # rate is learning speed, lower is better, but too slow will cause longer time. \n",
    "  library(rpart)\n",
    "  max_depth=1 # tree stump\n",
    "  mu = mean(y) # start with an initial model\n",
    "  dy = y - mu  # residuals or error,  These are the parts that existing model cannot do well.\n",
    "  learner = list() # define a learners holder \n",
    "  for (i in 1:iter) {\n",
    "    # use a weak model to improve\n",
    "    learner[[i]] = rpart(dy~x, # error is target variable\n",
    "                         control=rpart.control(maxdepth=max_depth,cp=0)) # cp=0 means to growth a tree with any cost\n",
    "    # modify residuals for next iter\n",
    "    dy = dy - rate*predict(learner[[i]])\n",
    "  }\n",
    "  result = list('learner'=learner,\n",
    "                'rate'=rate,\n",
    "                'iter'=iter)\n",
    "  return(result)\n",
    "}\n",
    "\n",
    "\n",
    "model = GBR(data$train_x,data$train_y,iter=1000)\n",
    "objects(model)\n",
    "\n",
    "# predict function\n",
    "predict_GBR = function(x,y,model){\n",
    "  predict_y = list() # hold predict result\n",
    "  mu = mean(y) # initial model\n",
    "  n = length(y)\n",
    "  iter = model$iter\n",
    "  rate = model$rate\n",
    "  predict_y[[1]] = rep(mu,n)\n",
    "  learner = model$learner\n",
    "  for (i in 2:iter) {\n",
    "    # add pre-model prediction to predict y\n",
    "    predict_y[[i]] = predict_y[[i-1]] + rate * predict(learner[[i-1]],newdata=data.frame(x))\n",
    "  }\n",
    "  # mean sqare error\n",
    "  mse = sapply(predict_y,function(pre_y) round(mean((y-pre_y)^2),3))\n",
    "  result = list('predict_y'=predict_y,\n",
    "                'mse'= mse)\n",
    "  return(result)\n",
    "}\n",
    "\n",
    "# predict train data\n",
    "predict_train = predict_GBR(data$train_x,data$train_y,model=model)\n",
    "objects(predict_train)\n",
    "\n",
    "# more trees get less error\n",
    "plot(predict_train$mse,type='l')\n",
    "\n",
    "\n",
    "# plot the effect of boosing tree\n",
    "plotfunc = function(x,y,predict,num){\n",
    "  library(ggplot2)\n",
    "  pre = predict$predict_y[[num]] \n",
    "  plotdf = data.frame(x=x,y=y,pre = pre)\n",
    "  mse = round(mean((y-pre)^2),3)\n",
    "  p = ggplot(plotdf,aes(x,y)) +\n",
    "    geom_point(alpha=0.5)\n",
    "  p = p + geom_line(aes(x,pre)) + xlab(paste0('mse=',mse))\n",
    "  plot(p)\n",
    "}\n",
    "\n",
    "# use mean of y to predict data\n",
    "plotfunc(data$train_x,data$train_y,predict_train,num=1)\n",
    "# add another tree model\n",
    "plotfunc(data$train_x,data$train_y,predict_train,num=2)\n",
    "# add more tree getter more result\n",
    "plotfunc(data$train_x,data$train_y,predict_train,num=10)\n",
    "plotfunc(data$train_x,data$train_y,predict_train,num=100)\n",
    "\n",
    "# predict test data\n",
    "predict_test = predict_GBR(data$test_x,data$test_y,model=model)\n",
    "plot(predict_test$mse,type='l')\n",
    "\n",
    "plotfunc(data$test_x,data$test_y,predict_test,10)\n",
    "plotfunc(data$test_x,data$test_y,predict_test,100)\n",
    "\n",
    "# compare different parametter\n",
    "# create 2 models with different rate\n",
    "model_1 = GBR(data$train_x,data$train_y,rate=1,iter=500)\n",
    "model_2 = GBR(data$train_x,data$train_y,rate=0.1,iter=500)\n",
    "\n",
    "# use train and test data, we have 4 results\n",
    "predict_train_1 = predict_GBR(data$train_x,data$train_y,model=model_1)\n",
    "predict_train_2 = predict_GBR(data$train_x,data$train_y,model=model_2)\n",
    "predict_test_1 = predict_GBR(data$test_x,data$test_y,model=model_1)\n",
    "predict_test_2 = predict_GBR(data$test_x,data$test_y,model=model_2)\n",
    "\n",
    "# take out mse of these 4 results\n",
    "train_error_1 = predict_train_1$mse\n",
    "train_error_2 = predict_train_2$mse\n",
    "test_error_1 = predict_test_1$mse\n",
    "test_error_2 = predict_test_2$mse\n",
    "iter = 1:model_1$iter\n",
    "\n",
    "# compare these mse\n",
    "plotdf = data.frame(iter,train_error_1,test_error_1,train_error_2,test_error_2)\n",
    "\n",
    "p = ggplot(plotdf)+\n",
    "    geom_line(aes(x=iter,y=train_error_1),color='blue')+\n",
    "    geom_line(aes(x=iter,y=test_error_1),color='red') +\n",
    "  geom_line(aes(x=iter,y=train_error_2),linetype=2,color='blue')+\n",
    "  geom_line(aes(x=iter,y=test_error_2),linetype=2,color='red')\n",
    "print(p)\n",
    "\n",
    "# test error is better model performance.\n",
    "# less rate is better but need more iter\n",
    "which.min(train_error_1)\n",
    "which.min(train_error_2)\n",
    "which.min(test_error_1)\n",
    "which.min(test_error_2)\n",
    "\n",
    "# 2. Gradient Boosting for Classification\n",
    "\n",
    "# read data\n",
    "data = subset(iris,Species!='virginica',select = c(1,2,5))\n",
    "data$y = ifelse(data$Species == 'setosa',1,-1)\n",
    "data$Species = NULL\n",
    "names(data)[1:2] = c('x1','x2')\n",
    "head(data)\n",
    "p = ggplot(data,aes(x1,x2,color=factor(y)))+\n",
    "    geom_point()\n",
    "print(p)\n",
    "\n",
    "# train boosting tree for classification\n",
    "GBC = function(data,rate=0.1,iter=100){\n",
    "  library(rpart)\n",
    "  max_depth=1\n",
    "  learner = list()\n",
    "  mu = mean(data$y==1)\n",
    "  # start with an initial model\n",
    "  # mu=p(y=1) -> f=w*x = log(mu/(1-mu)) \n",
    "  f = log(mu/(1-mu)) \n",
    "  data$dy = data$y/(1+exp(data$y*f)) # dy is negtive gradient of log loss funtion\n",
    "  for (i in 1:iter) {\n",
    "    # use a weak model to improve\n",
    "    learner[[i]] = rpart(dy~x1+x2,data=data,\n",
    "                         control=rpart.control(maxdepth=max_depth,cp=0))\n",
    "    # improve model \n",
    "    f = f + rate *predict(learner[[i]])\n",
    "    # modify dy\n",
    "    data$dy = data$y/(1+exp(data$y*f))\n",
    "  }\n",
    "  result = list('learner'=learner,\n",
    "                'rate'=rate,\n",
    "                'iter'=iter)\n",
    "  return(result)\n",
    "}\n",
    "\n",
    "model = GBC(data,rate=0.1,iter=1000)\n",
    "objects(model)\n",
    "\n",
    "# predict function\n",
    "predict_GBC = function(data,model){\n",
    "  predict_y = list()\n",
    "  mu = mean(data$y==1)\n",
    "  f = log(mu/(1-mu)) \n",
    "  n = nrow(data)\n",
    "  iter = model$iter\n",
    "  rate = model$rate\n",
    "  predict_y[[1]] = rep(f,n)\n",
    "  learner = model$learner\n",
    "  for (i in 2:iter) {\n",
    "    predict_y[[i]] = predict_y[[i-1]] + rate *predict(learner[[i-1]],newdata=data)\n",
    "  }\n",
    "  mse = sapply(predict_y,function(pre_y) sum(log(1+exp(-data$y*pre_y)))) # logistic loss function\n",
    "  result = list('predict_y'=predict_y,\n",
    "                'mse'= mse)\n",
    "  return(result)\n",
    "}\n",
    "\n",
    "# predict data\n",
    "predict_train = predict_GBC(data,model=model)\n",
    "objects(predict_train)\n",
    "plot(predict_train$mse,type='l')\n",
    "\n",
    "final = predict_train$predict_y[[1000]]\n",
    "y_p = 1/(1+exp(-final))\n",
    "y_pred = ifelse(y_p>0.5,1,-1)\n",
    "table(data$y, y_pred)\n",
    "\n",
    "# # plot\n",
    "\n",
    "plotfunc2 = function(data,num,rate=0.1){\n",
    "  library(ggplot2)\n",
    "  model = GBC(data,rate=rate,iter=num)\n",
    "  predict_train = predict_GBC(data,model=model)\n",
    "  final = predict_train$predict_y[[num]]\n",
    "  y_p = 1/(1+exp(-final))\n",
    "  data$y_pre = ifelse(y_p>0.5,1,-1)\n",
    "  tree_f = sapply(model$learner,function(x){\n",
    "    temp = x$splits\n",
    "    return(row.names(temp)[1])\n",
    "  })\n",
    "  tree_v = sapply(model$learner,function(x){\n",
    "    temp = x$splits\n",
    "    return(temp[1,'index'])\n",
    "  })\n",
    "  x1value = tree_v[tree_f=='x1']\n",
    "  x2value = tree_v[tree_f=='x2']\n",
    "  p = ggplot(data,aes(x=x1,y=x2))\n",
    "  p = p + geom_point(aes(color=factor(y_pre)),size=5) +\n",
    "    geom_point(aes(color=factor(y)),size=3)+\n",
    "    geom_vline(xintercept = x1value,alpha=0.4) +\n",
    "    geom_hline(yintercept = x2value,alpha=0.4) +  \n",
    "    scale_colour_brewer(type=\"seq\", palette='Set1') +\n",
    "    theme_bw()\n",
    "  print(p)\n",
    "}\n",
    "\n",
    "plotfunc2(data,num=10)\n",
    "plotfunc2(data,num=20)\n",
    "plotfunc2(data,num=60)\n",
    "plotfunc2(data,num=100)\n",
    "plotfunc2(data,num=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
